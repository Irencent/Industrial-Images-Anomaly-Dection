## DINO v2 瓶颈

**密集特征(局部、像素级特征)退化：**在长时间训练大参数量的模型时，高层语义理解和底层密集特征会发生冲突，导致后者崩溃

### 主要贡献

1. 引入 Gram 锚定：有效解决密集特征退化
2. 新颖的改变：使用模型自身在训练早期的版本作为学生的教师。

## Training at Scale Without Supervision

通过将模型和数据集大小提升一个数量级，产生通用、无偏见的视觉特征。

### 数据准备

从 Instagram 上先搜集了 170 亿张原始图片，并以此为基础构建了由三个部分组成的数据集。

1. 第一部分通过 DINOv2 提取特征，利用 k-means 进行聚类，得到数据池中所有图片类别。通过在数据中平衡采样，可以得到大约17亿张图片
2. 第二部分基于检索。先选定一些种子训练集，对于这些种子集中每张图片，在 170 亿的数据池中检索与之最相近的几张图片。
3. 第三部分加入了高质量的公开数据集， 如ImageNet1k。

**混合批次训练：**

每一次迭代中，10% 概率只在 ImageNet1k 上进行训练，90% 概率只接收来自其它数据源的异质化批次

### Large-Scale Training with Self-Supervision

**损失函数：**

综合 DINO loss，iBOT loss，以及Koleo 正则化

**模型架构更新**

1. RoPE位置编码，引入了 RoPE-box jittering: 在训练中，随机地缩放位置坐标的范围。
2. 提升了模型的参数量。

**optimization**

采用恒定的学习率、恒定的权重衰减和恒定的教师EMA动量

## Gram Anchoring

当 cls token 和 patch token 之间的模型相似度较低时，模型的分割性能较高。

随着训练的进行，cls token 越能代表整张图片即与各个图像块token 总余弦相似度越高，此时各个图像块 token 也趋同，即蕴含更多全局信息，而损失了局部信息。

### Gram 算法

核心思想是：加入一个新的 loss，这个 loss 不干预特征本身，但是要保证特征之间的相似性结构保持稳定。

**Gram 矩阵：**保存了图像中每一个块和其它块的相似度。新的目标即为让学生模型去模仿老师输出的 Gram 矩阵，这个老师即为学生训练较早时，密集特征尚未下降时的模型。

这个新损失函数在模型迭代了 100万次后才引进，称为**精炼阶段**。

### Leveraging Higher-Resolution Features

在精炼阶段，将高分辨率的图片输入进 Gram 老师网络，得到一张高分辨率的特征图，再通过降采样使得该特征图与学生网络输出的特征图尺寸一致。

## Post-training
