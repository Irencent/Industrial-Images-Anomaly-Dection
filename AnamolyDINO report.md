## CLIP

通过对比学习方法，建立图片和文字之间的联系。

## DINO

内部网络由教师和学生两个网络构成。

训练时，教师网络接收原始图片；对原始图片进行数据增强（裁剪，放大，旋转等），输入到学生网络中。

学生网络的训练目标是使得其输出与教师网络的输出尽可能接近。

通过这种“强迫自己从不同角度看图片，并得到一致性结论的”方法，模型学会了稳健的图片特征。它能理解图片的本质，并且关注到图像的细节特征。

## AD Methods

### Deep Nearest Neighbor

利用强大的特征提取器提取正常图片的特征并且存放到内存库中。

当检测新图片时，利用特征提取器提取特征，然后计算它与内存库中所有特征的距离，如果最短距离都很远，则判定为异类。

由全局性检测发展到图像块级检测。

**基于语言-视觉模型方法**

利用 CLIP 这种多模态大模型。

检测图片时，只需提供图片文字对，然后根据匹配程度判断图片是否属于异常。

### MutiModal chatbots

利用聊天机器人，用对话方式，让大模型直接给出结果。

## AnomalyDINO 工作原理

 **构建一个图像块特征内存库：**

首先将正常图片分割成patches。

利用 DINOv2, 对每个patch提取特征，存储到内存库当中。

**检测过程：**

检测新图片时，用同样的方式将其分为图像块。

计算每个图像块到内存库中所有特征的最短距离。

整合所有图像块的最短距离，得到一个总的分数：计算距离最高的前1%的平均距离。

**定位异常区域：**

将每个patch的距离作为该块的像素值，可以得到一张热力图。

利用双线性上采样将该低分率图放大到和原图一样大，再用高斯平滑处理，使得图片看起来更自然，从而可以标注出异常的位置。

## Enriching the Memory Bank & Filtering Relevant Patches

### Masking

通过将图片中物体和背景分开来，让模型只关注物体本身。

利用 DINOv2 对每个图像块提取特征，然后进行主成分分析，对第一个分量设定一个阈值。

**问题**

对于特写镜头（物体占据了图片的50%以上），用该方法无法分离。

**解决：掩码测试**

对于某一类，先只对该类的第一个样本做掩码，如果能够区分，则该类都进行掩码处理，否则不进行。

### Rotation

对于旋转不变性的物体非常有效，但是有些物体的旋转本身就是一种异常，则不能应用旋转。

## Experiment

### 性能评估指标

#### 图像级检测

衡量模型在判断“图片本身是否异常”的能力。

AUROC：

AP：平均精度

F1-max：

#### 像素级检测

衡量模型“精确定位出异常像素块”的能力。

AUROC：

F1-max：

PRO（per-region overlap）:计算模型预测的异常区域与真实的异常区域的重叠程度

#### 特别说明

不能仅仅用 AUROC 作为评判指标，因为数据本身就是失衡的。

### Results

#### 在 MVTec-AD 上

包含 15 个类别：主要描述物体或者纹理

标注：异常类型划分清晰，每个类不超过 8 种异常

**图像级**

在 1 样本、2样本、4样本、... 、16 样本设定下，模型均达到 SOTA

**像素级**

性能同样卓越。并且发现提高图像分辨率虽然对图像检测没有显著影响，但是对像素级分割有显著提高。

#### 在 VisA 数据集上

包含 12 个类别

标注：仅仅区分是否为异常

分辨率更高

**图像级**

低样本性能排第二

高样本性能为SOTA

**像素级分割**

性能为 SOTA

## 消融实验

### **1. 推理时间 (Inference Time)**

* **核心结论：** AnomalyDINO  **速度极快** 。
* **性能对比：** 在1样本设定下，AnomalyDINO 的推理速度**显著快于**其他顶尖的小样本竞争对手（图表使用了对数坐标轴，意味着实际差距非常大）。例如，`AnomalyDINO-S`（小型模型）处理一张 448 分辨率的图片大约只需要  **60毫秒** 。
* **速度与性能的权衡：** 只有 PatchCore 和使用 ImageNet 预训练骨干的模型比它更快，但这些方法在 **性能上做出了牺牲** 。AnomalyDINO 在速度和性能之间取得了更好的平衡。
* **数据增强的影响：** 尽管数据增强（如旋转）会增加记忆库的大小，理论上会减慢最近邻搜索的速度，但实验发现，这种影响在实际中 **可以忽略不计** 。

### **2. 预处理 (Preprocessing)**

* **核心结论：** 合理的预处理能 **有效提升性能，且几乎不增加时间成本** 。
* **性能提升：** 与不使用预处理（掩码、旋转）的版本相比，加入了预处理的版本在 AUROC 指标上平均能 **提升约 2%** 。
* **分辨率的影响：** 预处理带来的好处似乎在**更高分辨率**的图片上更加明显。
* **不同策略对比：** 实验比较了两种预处理策略：‘Agnostic’（通用策略，总是旋转）和 ‘Informed’（知情策略，根据先验知识决定是否旋转）。结果发现， **‘Agnostic’ 通用策略的表现甚至略微优于 ‘Informed’ 知情策略** 。
* **旋转的负面影响：** 尽管旋转在理论上有可能对某些特定物体产生负面影响，但实验观察到这种情况 **非常罕见** 。

### **3. 聚合统计方法 (Aggregation Statistic)**

* **核心结论：** 论文中选择的聚合方法**优于**传统方法。
* **方法对比：** 论文默认使用的“取最异常的1%图像块的平均距离”的方法，其效果**优于**更标准的“取上采样并平滑后的最大距离值”的方法。这验证了他们设计选择的合理性。

### **4. 架构大小与选择 (Architecture Size/Choice)**

这是最有趣也最深刻的一部分，包含了多个重要发现：

* **DINOv2 vs. ImageNet 预训练模型：**
  * **结果：** 实验证实， **DINOv2 是一个远比 ImageNet 预训练模型更适合该任务的骨干网络** 。
  * **原因：** ImageNet 模型的特征质量较弱，并且与论文提出的掩码（Masking）方法不兼容。这个对比有力地证明了 DINOv2 的优越性。
* **不同 DINOv2 模型大小 (ViT-S, B, L) 的影响：**
  * **核心结论：**  **模型并非越大越好** 。
  * **性能差异：** 不同大小的模型之间性能差异并不悬殊。即使是最大的模型，虽然也达到了顶尖水平，但并不一定比最小的模型更好。
  * **“因地制宜”的发现：**
    * 在 **MVTec-AD** 数据集（场景相对简单，多为单个物体）上， **最小的模型 (ViT-S) 表现最佳** 。
    * 在 **VisA** 数据集（场景更复杂，常有多个物体）上， **更大的模型表现更好** 。
  * **深刻洞见：**对于简单任务，过于庞大的模型可能会“想太多”，反而不如小模型效果好；而对于复杂任务，则需要大模型更强的表征能力。
