## CLIP

通过对比学习方法，建立图片和文字之间的联系。

## DINO

内部网络由教师和学生两个网络构成。

训练时，教师网络接收原始图片；对原始图片进行数据增强（裁剪，放大，旋转等），输入到学生网络中。

学生网络的训练目标是使得其输出与教师网络的输出尽可能接近。

通过这种“强迫自己从不同角度看图片，并得到一致性结论的”方法，模型学会了稳健的图片特征。它能理解图片的本质，并且关注到图像的细节特征。

## AD Methods

### Deep Nearest Neighbor

利用强大的特征提取器提取正常图片的特征并且存放到内存库中。

当检测新图片时，利用特征提取器提取特征，然后计算它与内存库中所有特征的距离，如果最短距离都很远，则判定为异类。

由全局性检测发展到图像块级检测。

**基于语言-视觉模型方法**

利用 CLIP 这种多模态大模型。

检测图片时，只需提供图片文字对，然后根据匹配程度判断图片是否属于异常。

### MutiModal chatbots

利用聊天机器人，用对话方式，让大模型直接给出结果。

## AnomalyDINO 工作原理

 **构建一个图像块特征内存库：**

首先将正常图片分割成patches。

利用 DINOv2, 对每个patch提取特征，存储到内存库当中。

**检测过程：**

检测新图片时，用同样的方式将其分为图像块。

计算每个图像块到内存库中所有特征的最短距离。

整合所有图像块的最短距离，得到一个总的分数：计算距离最高的前1%的平均距离。

**定位异常区域：**

将每个patch的距离作为该块的像素值，可以得到一张热力图。

利用双线性上采样将该低分率图放大到和原图一样大，再用高斯平滑处理，使得图片看起来更自然，从而可以标注出异常的位置。
